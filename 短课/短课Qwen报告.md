### ğŸ“‹ **QwenVL ç¯å¢ƒéƒ¨ç½²å¾…åŠæ¸…å•**

#### ğŸ”¹ 1. è¿æ¥æœåŠ¡å™¨
- [x] å®‰è£… VSCode + Remote-SSH æ’ä»¶  
- [x] ä½¿ç”¨ SSH å‘½ä»¤è¿æ¥ AutoDL æœåŠ¡å™¨ 
  ```bash
  ssh -p <ç«¯å£> root@connect.bjb1.seetacloud.com
  ```
- [x] ç¡®è®¤è¿›å…¥ `/root/autodl-tmp` ç›®å½•  

---

#### ğŸ”¹ 2. å…‹éš†ä»£ç ä»“åº“
- [x] å…‹éš† Qwen3-VL å®˜æ–¹ä»“åº“ 
  ```bash
  git clone https://github.com/QwenLM/Qwen3-VL.git
  ```

---

#### ğŸ”¹ 3. å®‰è£…åŸºç¡€ä¾èµ–
- [x] å®‰è£… transformersï¼ˆâ‰¥4.57.0ï¼‰ 
  ```bash
  pip install "transformers>=4.57.0"
  ```
- [x] å®‰è£… qwen-vl-utils  
  ```bash
  pip install qwen-vl-utils==0.0.14
  ```
- [x] å®‰è£… accelerate 
  ```bash
  pip install accelerate
  ```

---

#### ğŸ”¹ 4. å®‰è£… flash-attnï¼ˆåŠ é€Ÿè®­ç»ƒ/æ¨ç†ï¼Œå¯é€‰ä½†æ¨èï¼‰
- [ ] å°è¯•ç›´æ¥å®‰è£…ï¼š  
  ```bash
  pip install -U flash-attn --no-build-isolation
  ```
- [x] è‹¥å¤±è´¥ â†’ ä¸‹è½½å¯¹åº” `.whl` æ–‡ä»¶ï¼ˆ[é¢„ç¼–è¯‘ wheel åœ°å€](https://github.com/mjun0812/flash-attention-prebuild-wheels/releases)ï¼‰ 
- [ ] ä¸Šä¼  `.whl` åˆ°æœåŠ¡å™¨å¹¶å®‰è£…ï¼ˆç¤ºä¾‹ï¼‰ï¼š  
  ```bash
  pip install /root/autodl-tmp/flash_attn-*.whl --no-build-isolation
  ```

---

#### ğŸ”¹ 5. éƒ¨ç½² ms-swift æ¡†æ¶ï¼ˆç”¨äºåç»­å¾®è°ƒï¼‰
- [x] å…‹éš† ms-swift ä»“åº“ 
  ```bash
  git clone https://github.com/modelscope/ms-swift.git
  ```
- [x] å®‰è£… ms-swiftï¼ˆå¼€å‘æ¨¡å¼ï¼‰ 
  ```bash
  cd ms-swift && pip install -e .
  ```

---

#### ğŸ”¹ 6. ä¸‹è½½æ¨¡å‹æƒé‡
- [x] å®‰è£… huggingface å·¥å…· 
  ```bash
  pip install -U huggingface_hub hf-cli
  export HF_ENDPOINT=https://hf-mirror.com
  ```
- [x] é€šè¿‡ CLI ä¸‹è½½æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼‰ï¼š
  ```bash
  hf download Qwen/Qwen3-VL-4B-Instruct --local-dir /root/autodl-tmp/models/Qwen3-VL-4B-Instruct
  ```
  > æˆ–æ‰‹åŠ¨ä» [HF-Mirror](https://hf-mirror.com) æˆ– [é­”æ­](https://www.modelscope.cn) ä¸‹è½½åç”¨ Xftp ä¸Šä¼ 


---

### ğŸ“‹ **å•å›¾ Zero-Shot æ¨ç†æµ‹è¯•å¾…åŠæ¸…å•ï¼ˆQwen3-VL-4Bï¼‰**

#### ğŸ”¹ 1. ç¡®è®¤å‰ææ¡ä»¶
- [x] å·²æˆåŠŸéƒ¨ç½² Qwen3-VL ç¯å¢ƒï¼ˆå« `transformers`ã€`qwen-vl-utils`ã€`accelerate`ï¼‰ 
- [x] å·²ä¸‹è½½æ¨¡å‹æƒé‡è‡³æœ¬åœ°è·¯å¾„ 
  ç¤ºä¾‹ï¼š`/root/autodl-tmp/models/Qwen3-VL-4B-Instruct`
  ![[Pasted image 20251120002223.png]]
- [x] æµ‹è¯•å›¾åƒå·²ä¸Šä¼ åˆ°æœåŠ¡å™¨ 
  ç¤ºä¾‹ï¼š`/root/autodl-tmp/data/

---

#### ğŸ”¹ 2. å‡†å¤‡æ¨ç†è„šæœ¬
##### âœ… è‡ªå®šä¹‰ Python è„šæœ¬
- [x] åœ¨ `/root/autodl-tmp/` ä¸‹æ–°å»ºæ–‡ä»¶ `test_single_image.py` 
- [x] å°†ä»¥ä¸‹å†…å®¹ç²˜è´´è¿›å»ï¼ˆå·²é€‚é… 4B æ¨¡å‹ + JSON è¾“å‡ºè¦æ±‚ï¼‰ï¼š 

```python
from transformers import AutoModelForImageTextToText, AutoProcessor
import torch
model_path = "/root/autodl-tmp/models/Qwen3-VL-4B-Instruct"

# default: Load the model on the available device(s)

model = AutoModelForImageTextToText.from_pretrained(
    model_path,
    dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    device_map="auto",
 )

processor = AutoProcessor.from_pretrained(model_path)

query = """
æˆ‘ä¼šæä¾›ä½ ä¸€å¼ è¯ç…§å›¾åƒï¼Œè¿™å¼ å›¾åƒä¸Šå¯èƒ½å­˜åœ¨ä¸€äº›æ±¡æŸï¼Œè¯·ä½ å¸®æˆ‘åˆ†æè¿™å¼ å›¾åƒä¸Šæœ‰ä»€ä¹ˆæ±¡æŸï¼Ÿ
æˆ‘å¯¹æ±¡æŸçš„å®šä¹‰æ˜¯ï¼šå›¾åƒä¸Šå­˜åœ¨çš„ä»»ä½•å½±å“è¯ç…§ä¿¡æ¯è¯†åˆ«çš„ç¼ºé™·æˆ–æŸåï¼Œå®ƒå¯èƒ½æ˜¯ç‰©ç†å±‚æ¬¡çš„ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯æ•°å­—å±‚æ¬¡çš„ã€‚
è¯·ä½ ç”¨jsonæ ¼å¼å›ç­”ï¼Œå›ç­”ä¸­åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
"æ±¡æŸç±»å‹"ï¼š"æè¿°æ±¡æŸçš„ç±»å‹ï¼Œä¾‹å¦‚åˆ’ç—•ã€æ±¡æ¸ã€æŠ˜ç—•ç­‰",
"æ±¡æŸä½ç½®"ï¼š"æè¿°æ±¡æŸåœ¨å›¾åƒä¸Šçš„å…·ä½“ä½ç½®ï¼Œä¾‹å¦‚å·¦ä¸Šè§’ã€å³ä¸‹è§’ç­‰",
"æ±¡æŸä¸¥é‡ç¨‹åº¦"ï¼š"æè¿°æ±¡æŸçš„ä¸¥é‡ç¨‹åº¦ï¼Œä¾‹å¦‚è½»å¾®ã€ä¸­ç­‰ã€ä¸¥é‡ç­‰",
"åˆ¤æ–­åŸå› "ï¼š"æè¿°ä¸ºä»€ä¹ˆå°†è¯¥ä½ç½®åˆ¤æ–­ä¸ºæ±¡æŸ"
}
"""
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "/root/autodl-tmp/data/damaged/finger_occlusion/finger_occlusion_1.jpg",
            },
            {"type": "text", "text": query},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```
---

#### ğŸ”¹ 3. æ‰§è¡Œæ¨ç†
  ```bash
  python /root/autodl-tmp/test_single_image.py
  ```


---

#### ğŸ”¹ 4. è¾“å‡ºç»“æœ
![[Pasted image 20251120011559.png]]
![[Pasted image 20251120011533.png]]
æ˜æ˜¾æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ‰‹é®æŒ¡çš„æƒ…å†µã€‚



---

## âœ… Qwen3-VL å¾®è°ƒå¾…åŠæ¸…å•ï¼ˆSFTï¼‰

### 1. ã€æ•°æ®å‡†å¤‡ã€‘
- [x] **æ”¶é›†/ç”Ÿæˆå›¾åƒæ•°æ®** 
  - ä½¿ç”¨çœŸå®è¯ç…§å›¾åƒ + è‡ªè¡Œä¼ªé€ /æ±¡æŸå›¾åƒï¼ˆå¦‚å§“åç¯¡æ”¹ã€æ°´æ¸è¦†ç›–ç­‰ï¼‰
- [x] **æ ‡æ³¨æ•°æ®** 
  - æ ¼å¼ï¼šJSON æ–‡ä»¶ï¼Œæ¯æ¡æ ·æœ¬ä¸ºä¸€ä¸ª dictï¼Œå« `messages` å’Œ `images` å­—æ®µ  
    ç¤ºä¾‹ï¼š
    ```json
    {
      "messages": [
        {"role": "user", "content": "<image>è¿™å¼ è¯ä»¶æ˜¯å¦è¢«ç¯¡æ”¹ï¼Ÿ"},
        {"role": "assistant", "content": "æ˜¯çš„ï¼Œå§“åå­—æ®µè¢«ä¿®æ”¹è¿‡ã€‚"}
      ],
      "images": ["/root/autodl-tmp/data/fake_id_001.jpg"]
    }
    ```
  - å¯é€‰æ ‡æ³¨ç­–ç•¥ï¼š
    - äººå·¥æ ‡æ³¨ï¼ˆé«˜ç²¾åº¦ï¼‰
    - å¤§æ¨¡å‹è¾…åŠ©æ ‡æ³¨ï¼ˆç”¨ zero-shot æ¨¡å‹æ‰¹é‡ç”Ÿæˆåˆç¨¿å†æ ¡å¯¹ï¼‰
- [x] **åˆ›å»ºç›®å½•ç»“æ„** âœ…
  ```bash
  mkdir -p /root/autodl-tmp/data
  mkdir -p /root/autodl-tmp/sft_output
  mkdir -p /root/autodl-tmp/script
  ```
- [x] åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›† 
>æ±¡æŸä»¥ç²‰ç¬”ç°å±…å¤š
![[Pasted image 20251120012656.png]]

![[Pasted image 20251120012858.png]]

---


### 2. ã€ç¼–å†™è®­ç»ƒè„šæœ¬ã€‘
- [x] åœ¨ `/root/autodl-tmp/script/` ä¸‹æ–°å»º `train_sft.sh`
- [x] å¡«å…¥ä»¥ä¸‹æ¨¡æ¿ï¼ˆæ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰ï¼š 
  ```bash
  #!/bin/bash
  CUDA_VISIBLE_DEVICES=0 swift sft \
    --model "/root/autodl-tmp/models/Qwen3-VL-8B-Instruct" \
    --dataset "/root/autodl-tmp/data/train.json" \
    --output_dir "/root/autodl-tmp/sft_output" \
    --num_train_epochs 3 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-4 \
    --max_length 2048 \
    --lora_rank 64 \
    --lora_alpha 128 \
    --lora_dropout 0.05 \
    --split_dataset_ratio 0.1  # è‡ªåŠ¨åˆ’åˆ†10%ä½œä¸ºéªŒè¯é›†
  ```
- [x] èµ‹äºˆæ‰§è¡Œæƒé™ï¼š 
  ```bash
  chmod +x /root/autodl-tmp/script/train_sft.sh
  ```

---

### 4. ã€å¯åŠ¨å¾®è°ƒè®­ç»ƒã€‘
- [x] è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š âœ… 2025-11-20
  ```bash
  cd /root/autodl-tmp/script
  bash train.sh
  ```

---
![[Pasted image 20251120015651.png]]
![[Pasted image 20251120015704.png]]

### 5. ã€å¾®è°ƒåæ¨ç†æµ‹è¯•ã€‘
- [x] ç¼–å†™æ¨ç†è„šæœ¬ `infer_finetuned.sh`ï¼š âœ… 2025-11-20
  ```bash
 #!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments=True"
export CUDA_VISIBLE_DEVICES=0
export IMAGE_MAX_TOKEN_NUM=1024
export FPS_MAX_FRAMES_NUM=16

swift infer \
  --max_pixels 3010560 \
  --adapters /root/autodl-tmp/models/sft_4B_qwenvl/v1-20251029-155338/checkpoint-267 \
  --stream true \
  --max_new_tokens 2048 \
  --load_data_args false \
  --val_dataset /root/autodl-tmp/models/sft_4B_qwenvl/v1-20251029-155338/val_dataset.jsonl \
  --result_path /root/autodl-tmp/models/sft_4B_qwenvl/v1-20251029-155338/test_output_1029_1.jsonl
  ```  
ğŸ”§ èµ‹äºˆæ‰§è¡Œæƒé™å¹¶è¿è¡Œï¼š
```
chmod +x /root/autodl-tmp/script/infer.sh
cd /root/autodl-tmp/script
./infer.sh
```

åˆ†åˆ«å¯¹ä¸‰ä¸ªè®­ç»ƒäº§ç”Ÿçš„æ¨¡å‹è¿›è¡Œinfer:
![[Pasted image 20251120023246.png]]
è¿›è¡ŒéªŒè¯é›†æ‰“ä¹±ï¼š
- éªŒè¯é›†é€šå¸¸**åªåœ¨è®­ç»ƒå¼€å§‹å‰æ‰“ä¹±ä¸€æ¬¡**ï¼ˆæˆ–å®Œå…¨ä¸æ‰“ä¹±ï¼‰
- å½“ä½ ä½¿ç”¨Â `--save_steps 100`Â å®šæœŸä¿å­˜å¹¶è¯„ä¼°æ—¶
- æ—©æœŸè¯„ä¼°å¯èƒ½åª"çœ‹åˆ°"éªŒè¯é›†ä¸­ç®€å•çš„æ ·æœ¬
- åæœŸè¯„ä¼°"çœ‹åˆ°"æ›´éš¾çš„æ ·æœ¬ï¼Œå¯¼è‡´å‡†ç¡®ç‡**è™šå‡ä¸‹é™**



![[Pasted image 20251120024212.png]]

![[Pasted image 20251120025030.png]]

![[Pasted image 20251120030925.png]]


