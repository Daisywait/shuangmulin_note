---
类型: 概念
标签: [深度学习, Transformer]
---

来源[[shuangmulin/20_参考资料/深度学习基础理论/Transformer]]

- 一句话：多头自注意力 + 前馈网络 + 残差与层归一化。
- 关键：多头注意力建模全局依赖。
- 误区：只堆层不一定稳定，需残差与归一化。
- 例子：BERT/ViT 编码器骨架。
- 链接






