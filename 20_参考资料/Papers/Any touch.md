## 第一遍
标题：ANYTOUCH: LEARNING UNIFIED STATIC-DYNAMIC  REPRESENTATION ACROSS MULTIPLE VISUO-TACTILE  SENSORS
    多视觉 - 触觉传感器学习统一的静态 - 动态表征
摘要：
	为了让机器人像人一样精准操控和感知物品，有许多触觉-视觉传感器被集成到机器人身上，但是由于诸多传感器之间并无统一的标准，它们各自拥有独特的特征表示，这个成为了本论文试图解决的问题，即实现一个统一视觉-触觉传感器的表征，（也可以叫跨传感器表征学习）这个表征还被提议要同时具有动态和静态的特点，为了此研究，作者团队还构建了TacQuad数据集（同一个物体、同一个接触、不同传感器的数据），提出了一个any touch框架即论文标题
结论：结论很简短，就是总结了一番论文中的主要两块内容，一就是TacQuad数据集，二是any touch框架。
## 第二遍
#### 阅读步骤：
第二遍里面我们就要对整个文章完整过一遍，然后知道每一块到底在干什么东西，我们可以沿着从标题一直往下读到最后，但是这个时候
也不需要注意太多的细节，以及一些公式的证明等等。
关注的地方
第二遍阅读的时候，最重要是搞明白那些重要的*图和表*，都要知道他每一个字在干什么事情
作者提出的方法和别人提出的方法是怎么进行对比的？之间差距有多大？这个时候可能你还没有特别搞懂他在干什么。但是不要紧，你可以将*不懂的地方标记下来，留到之后第三遍进行阅读*
达到的效果
第二遍阅读完之后，*你就对整个论文的各个部分，都有一个大概的了解*，中间可以把作者引用的别人的相关文献圈出来，比如作者是在某某某的方法上进行了*改进*，做了哪些改进之类的。
#### 1.intro:
###### 背景
第一段也是简单介绍了一下背景，然后指出
>This variability poses challenges to building precise robotic tactile systems, as sensor-specific data collection (Yang et al., 2022; Gao et al., 2023) and model training limit the data scale and diversity for the model of a single sensor and lead to suboptimal perception capabilities

传感器的多种多样和差异化的特征数据让机器人触觉系统的构建增加了挑战，因为若要是针对单个传感器进行数据的收集会面临以下问题：
- 成本高（每换一种传感器都得重新采集和标注）；
- 数据量有限；
- 无法覆盖多种材质、形态和环境。
因为特定于传感器的数据收集（Yang 等人，2022；Gao 等人，2023）和模型训练限制了单个传感器模型的数据规模和多样性，并导致感知能力欠佳。
###### 之前的研究
1.2024年的[[T3]]提出用“tokenization（符号化）”机制，把不同传感器的原始触觉信号转换成一种**统一的“触觉语言”**。属于“sample-level alignment”（样本级对齐）
- **是对齐的，但有限**：它只有少量样本在多个传感器之间是配对的（同一物体、同一接触条件），数量级远远不够覆盖所有物体、材质和动作。
- **覆盖的模态少**：主要是 GelSight、VisGel 等光学触觉或部分压力阵列传感器。
- **对齐方式偏样本级**：每一对样本明确对应，但没有大规模、多样化、多传感器的数据覆盖。
>改进：本研究引进了动态的表征，提供了TacQuad多模态对齐的数据集，且属于语义层面的对齐

考虑到数据集的问题：
2.2024提出双传感器对齐，罗德里格斯等人（2024 年）收集了一个双传感器配对数据集，以实现跨传感器生成。然而，他们对特定操作任务的关注限制了传感器和所收集物体的多样性。此外，他们忽视了配对的多模态数据在提高传感器可转移性和实现全面触觉感知方面的潜在益处。
- 他们尝试收集了**配对数据**：同一个物体在**两种传感器**下的触觉信号。
- 目的是让模型能做**跨传感器生成**（cross-sensor generation），即用一种传感器的数据预测另一种传感器的信号
局限：
- **操作任务有限**：他们只关注特定的触摸或操作类型，比如抓握、滑动等。
- **传感器和物体种类少**：数据集覆盖的触觉传感器类型和物体材质不多，导致训练的模型**泛化性受限**。

###### 本研究
数据集：TacQuad——an aligned multi-modal multisensor tactile dataset 对齐的多模态多传感器触觉数据集
- 多样性：①72,606 contact frames；②four different visuo-tactile sensors（publicly available sensors, self-made sensors, and force field sensors）
- 降低收集成本：在校准平台上进行细粒度时空对齐的数据收集，更大规模的粗粒度空间对齐数据通过手持设备进行收集。
- 每个采集物标注触觉属性描述，从而构建一个全面的触觉 - 视觉 - 语言数据集。


表征模型如何构建：
>我们认识到，人类的触觉感知是静态和动态过程的结合，因为人类会从纹理、滑动和压力变化等多种类型的信息中获得全面的触觉感知。——动态和静态

Any touch:a unified staticdynamic multi-sensor tactile representation learning framework.
- 输入有图片也有视频，即包含静态和动态
- 多层次的架构：以全面增强模型捕捉像素级触觉细节（像素级）和与传感器无关特征（语义级）的能力。
- 掩码建模：以学习细粒度的像素级细节
- 多模态对齐和一项新颖的跨传感器匹配任务，以理解不同传感器下物体的语义级触觉属性，并提取与传感器无关的特征。
- 减小传感器之间的差异：通过让多种传感器表征共享同一个空间然后再根据它们自己代表的物品的触觉信息
- 多传感器之间的知识迁移：token 替换
	跨传感器泛化策略：token 替换（在训练时，每个传感器的数据通常会有一个专门的标识符 token，告诉模型“这是哪种传感器的数据”。
	策略：随机把这些特定 token 替换成统一的通用 token（universal sensor token）。
	作用：
	模型不能只依赖特定传感器的标识
	被迫学习 与传感器无关的触觉特征）
- 实验和表现：
	- **多数据集验证**：在不同触觉数据集上测试 AnyTouch 的跨传感器能力和泛化能力。
	- **真实实验**：做了 **细粒度倒水（fine-grained pouring）实验**，测试模型在现实操作中的感知能力。
	- 实验结果表明：AnyTouch 能同时捕捉静态和动态触觉信息，并能在不同传感器间迁移。

#### 2.Related work
###### 2.1跨领域学习
举了两个在此领域的两个方法：①对比学习，多语言（multi-source language）训练中，通过让语义相同的句子（比如英文和中文的同义句）在表示空间靠近，而语义不同的句子远离。②循环一致性：在风格迁移（比如把马变成斑马）中，输入一张图像到另一个域再转回来，要求得到的图像和原始图像一致。- 即：`A → B → A`，希望重建出来的 `A' ≈ A`。
之前的研究：
- multi-sensor joint training：让不同传感器（如触觉A、触觉B、触觉C）**在同一个模型里一起训练**，共享部分网络参数，从而学到通用表征。
- multi-modal alignment：对齐**不同模态之间的语义空间**，例如视觉和触觉、触觉和语言，让它们在同一个嵌入空间中可以互相理解。
- cross-sensor generation：让模型从一种传感器的数据**生成另一种传感器的数据**。
>存在的问题：overlook the benefits of jointly utilizing multi-modal data and aligned multi-sensor data to bridge the sensor gap.

###### 2.2视触觉传感器的应用
被用于机器人的灵巧操作比如密集堆积、抓取、插入操作；与其他传感器协作，实现*动态*整合，比如pouring操作，还有peg insertion with keyway（带方向约束的插销动作，类似于插U盘）；也有静态的：比如材料分类和形状重建
>存在的问题：However, due to the low standardization of visual-tactile sensors, these methods fail to leverage larger and more diverse data from other sensors and lack sensor transferability
###### 2.3表征学习
定义：
	- **将原始复杂数据映射到一个有意义的向量空间；**
	-  **让相似的输入（比如两种橡胶材质）在空间中靠近；**
	- **不同的输入（比如金属 vs 布料）在空间中远离。**
举例：
背景：表征学习（自监督方法如 [[BERT]]/[[MAE]]）在视觉、语言和其他模态上取得成功，如今正扩展到多模态领域，触觉也可以通过图像化处理（通过视触觉传感器），用类似视觉的方法进行表征学习，从而提升触觉模型在多任务、多传感器环境下的表现。
>存在的问题：However, these efforts have not explored how to obtain a unified visuo-tactile representation suitable for various tasks.

#### 3.TacQuad数据集
###### 3.1以前的研究：
罗德里格斯等人（2024 年）进行了初步尝试来解决这个问题，他们收集了一个包含 32256 对触觉图像的数据集，这些图像来自两个传感器，用于特定操作任务，但涉及的物体种类有限。该研究*没有考虑材料和硬度等触觉属性*，也*忽视了通过多模态信息增强跨传感器迁移能力*的潜力。
###### 如何解决：
提供多传感器对齐的数据，其中包含文本和图像，明确使模型能够学习*语义级别的触觉属性*(GPT-4o）和与传感器无关的特征（将原本的token替换成通用的），从而通过数据驱动的方法形成一个统一的多传感器表示空间。
###### 3.2数据收集：
前三个传感器用于收集触觉图像，而 Tac3D 用于捕捉变形力场。考虑成本问题但为了在更大规模上收集数据，同时确保尽可能多的数据配对，采用粗细两种方法来收集对齐数据![[Pasted image 20251112101355.png]]]
1.Fine-grained Spatio-temporal Aligned(
精细时空对齐)：
- 按压**一个玻璃罐**的顶部中心，该罐子表面坚硬、光滑、透明且具有一系列纹理。
- 有校准平台
- 四种传感器以相同的速度按压同一物体的相同位置。
- 25个物体，30次触摸 (Touches)，17,524帧。
2.Coarse-grained Spatial Aligned (粗粒度空间对齐）：
- 按压**一个粗糙的橙子**的顶部中心。接触点相对坚硬，有粗糙度和轻微的弹性。
- 手动采集
- 四种传感器按压同一物体上的**相同位置**，但**不保证时间上的同步对齐**。它包含了**室内和室外**场景。
- 99个物体，151次触摸 (Touches)，55,082帧。
3.数据分类：
- **Hard（硬）：** 塑料、金属、石头、玻璃等。
- **Soft（软）：** 织物、纸张、海绵等。
- **Food（食物）：** 蔬菜、水果等。

4.Cross-sensor Generation：跨传感器生成是指**利用一种传感器采集到的数据，来预测或合成（生成）另一种传感器在同一接触事件中本应采集到的数据**
5.Cross-sensor Matching：跨传感器匹配是指**确定来自不同传感器的数据是否描述了（或对应于）** **相同的** **物理触摸事件或物体。**
#### 4.Any Touch框架![[Pasted image 20251112104431.png]]]
围绕着这张图来展开：
###### 4.1整合图片和视频输入：
we consider tactile images as single-frame static videos to unify tactile images and videos![[Pasted image 20251113093712.png]]]![[Pasted image 20251113093727.png]]]![[Pasted image 20251113093800.png]]]
今天要补一下[[Public/深度学习/Transformer]]
###### 4.2pixel-Level
[[自监督学习]]






#### 5.使用的数据集：

以下是这九个数据集的详细介绍及其异同点的分析。

一、 预训练数据集概览 (Training Dataset Statistics)

|   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|
|编号|数据集名称|核心传感器|视觉模态 (Vision)|文本模态 (Text)|视频模态 (Video)|训练数据规模 (Frames)|
|1|**TacQuad**|GelSight, DIGIT, DuraGel, GelSight Mini|✔|✔|✔|55k|
|2|**Touch and Go (TAG)**|GelSight|✔|✔|✔|250k|
|3|**VisGel**|GelSight|✔|✘|✔|587k|
|4|**Cloth**|GelSight|✘*|✘|✔|587k|
|5|**ObjectFolder Real (OF Real)**|GelSlim|✔|✔|✔|1165k|
|6|**TVL**|DIGIT|✔|✔|✔|39k|
|7|**SSVTP**|DIGIT|✔|✔|✘|4.5k|
|8|**YCB-Slide**|DIGIT|✘*|✘|✔|183k|
|9|**Octopi**|GelSight Mini|✘|✔|✔|39k|

* * 注：Cloth 和 YCB-Slide **原始数据包含视觉模态**，但为了展示模型对模态缺失的兼容性，**本研究故意没有使用**其视觉模态。

二、 数据集详细介绍

1. TacQuad (本研究团队构建)

• **传感器：** GelSight、DIGIT、DuraGel（自制传感器）、GelSight Mini。TacQuad 是**唯一一个从四种不同视觉-触觉传感器收集的对齐的多模态多传感器触觉数据集**。

• **数据特点：** 总共包含 72,606 个接触帧。研究团队采集了两种对齐方式的数据：

    ◦ **精细粒度时空对齐数据：** 在校准平台上采集，精度高但成本高，包含 17,524 帧。

    ◦ **粗粒度空间对齐数据：** 通过手持方式采集，规模更大，包含 55,082 帧（用于训练）。

• **辅助模态：** 每个触觉帧都配有**视觉图像**和使用 **GPT-4o 生成并经过人工校正的触觉属性文本描述**。

2. Touch and Go (TAG)

• **传感器：** GelSight。

• **数据规模：** 250k 帧。

• **辅助模态：** 文本描述是由 **GPT-4o 模型生成**的。

3. VisGel

• **传感器：** GelSight。

• **数据规模：** 587k 帧。

• **模态：** 包含视觉和视频（动态感知），但**缺乏文本模态**。

4. Cloth

• **传感器：** GelSight。

• **数据规模：** 587k 帧。

• **模态：** 包含视频（动态感知），**缺乏视觉和文本模态**（视觉模态被有意排除）。

5. ObjectFolder Real (OF Real)

• **传感器：** GelSlim。

• **数据规模：** 1165k 帧，是训练数据集中规模最大的之一。

• **辅助模态：** 文本描述是由 **GPT-4o 模型生成**的。

6. TVL

• **传感器：** DIGIT。

• **数据规模：** 39k 帧。

• **辅助模态：** 原始数据集只有简单的短语级触觉描述，本研究团队使用 **GPT-4o 扩展**了其文本模态。

7. SSVTP

• **传感器：** DIGIT。

• **数据规模：** 4.5k 帧，是训练数据集中规模最小的。

• **模态：** 缺少视频模态（只包含静态触觉图像）。

• **辅助模态：** 原始数据集只有简单的短语级触觉描述，本研究团队使用 **GPT-4o 扩展**了其文本模态。

8. YCB-Slide

• **传感器：** DIGIT。

• **数据规模：** 183k 帧。

• **模态：** 包含视频（动态感知），**缺乏视觉和文本模态**（视觉模态被有意排除）。

9. Octopi

• **传感器：** GelSight Mini。

• **数据规模：** 39k 帧。

• **模态：** 缺少视觉模态。

• **辅助模态：** 原始数据集只有简单的短语级触觉描述，本研究团队使用 **GPT-4o 扩展**了其文本模态。

三、 数据集的相同点与不同点

这些数据集的整合是为了通过数据驱动的方法解决触觉感知系统的核心挑战：**低标准化**和**缺乏统一的表示空间**。

相同点 (Similarities)

1. **核心目标一致（触觉数据）：** 所有九个数据集都提供了用于训练模型的触觉图像或触觉视频（即主要模态 XT​），以学习静态和动态的触觉细节。

2. **统一的语义级辅助模态：** 除了 VisGel、Cloth 和 YCB-Slide 缺乏或有意排除了辅助模态外，所有具有或能够扩展文本模态的数据集（TacQuad, TAG, OF Real, TVL, SSVTP, Octopi）的**文本描述都使用了 GPT-4o 生成或扩展**。这使得所有数据集能够拥有一个**统一的、详细的语义级触觉属性描述**，从而充当对齐的“锚点”，帮助弥合传感器差距。

3. **支持静态和动态感知学习：** 绝大多数数据集（SSVTP 是例外，因为它缺少视频）都提供了连续帧，使得模型可以学习**静态属性**（如材质和纹理）和**动态变化**（如滑动和压力变化）。

不同点 (Differences)

1. **传感器类型多样性：** 这是最大的差异，也是研究需要解决的核心问题。

    ◦ 数据集覆盖了至少**五种不同的传感器**：GelSight、GelSlim、DIGIT、GelSight Mini 和 DuraGel（TacQuad 中）。
    ◦ TacQuad 是**唯一**一个明确提供**多传感器对齐数据**（即多个传感器触摸同一物体同一位置）的数据集，其余数据集大多是单传感器采集的。

2. **模态完整性：** 各数据集在辅助模态的完整性上存在显著差异。

    ◦ **三模态完整：** 只有 TacQuad、TAG、OF Real 和 TVL 拥有完整的触觉-视觉-文本-视频四重模态（其中 TacQuad, TAG, OF Real, TVL 的文本是经过本研究团队处理或生成后完整的）。
    ◦ **模态缺失：** VisGel（缺少文本），SSVTP（缺少视频），Octopi（缺少视觉），以及被故意移除视觉模态的 Cloth 和 YCB-Slide，都存在模态缺失情况。这导致研究需要设计**兼容模态缺失的多模态对齐方法**。

3. **数据规模和来源：** 数据集在规模上从 4.5k 帧（SSVTP）到 1165k 帧（OF Real）不等，且采集环境多样（例如，TacQuad 包含了室内和室外场景，以及精细与粗糙的采集方法）。

4. **数据代表性：** 传感器种类不同意味着它们感知触觉信息的方式和图像特征存在本质差异，这正是需要 AnyTouch 框架来**提取传感器无关特征**（sensor-agnostic features）的原因。例如，实验发现 DIGIT 的图像可能比 GelSight Mini 的图像与 GelSight 的图像差异更大，因此在知识迁移中带来的益处较少。


#### 训练和测试过程
##### 1.预训练基础设置和统一输入

1. **训练数据：** 使用**九个不同的触觉数据集**（总计约 248 万个接触帧）进行预训练。[[#5.使用的数据集：]]

2. **统一输入格式：** 为了统一处理静态触觉图像和动态触觉视频，研究团队将**静态图像**（I∈R1×H×W×3）沿着时间轴复制 F 次（例如 F=3 帧），从而将其视为**单帧静态视频**。无论是图像还是视频，最终都表示为一个统一的 4-D 张量 XT​∈RF×H×W×3。[[#4.1整合图片和视频输入：]]

3. 传感器标记 (Sensor Tokens)：为了促进知识迁移，引入了一组**可学习的传感器标记**（Sensor-specific tokens sk​）和一个**通用传感器标记**（Universal Sensor Token su​）。

4. **训练策略：** 在训练过程中，以 pu​ 的概率（从 0 线性增加到 0.75）用通用传感器标记替换特定传感器标记，这有助于模型整合和存储所有传感器的信息，以便更好地泛化到新传感器。

5. **实施细节：** 编码器基于 **OpenCLIP-Large**。整个训练过程交替使用触觉图像和视频片段进行训练。

###### 阶段一：像素级细节学习（Masked Modeling）

第一阶段专注于利用**掩码自编码器 (MAE)** 技术，从多传感器数据中捕捉精细的**像素级细节**和**时间动态变化**。

• **机制：** 对触觉图像和视频的标记（tokens）进行随机掩码（掩码比例 ρ=0.75）。

• **任务：**

    1. **静态图像重建 (**LrecS​**):** 重建被掩码的静态触觉图像，损失函数为像素空间中的均方误差（MSE）。

    2. **动态视频重建 (**LrecD​**):** 重建被掩码的动态触觉视频。

    3. **下一帧预测 (**LpredD​**):** 引入额外的任务，预测视频序列中的下一帧 VF+1​，以增强模型对连续形变变化的理解。

• **第一阶段总损失：** Lstage1​=LrecS​+LrecD​+LpredD​。

• **训练时长：** **20 个 epochs**。

###### 阶段二：语义级特征学习（对齐与匹配）

第二阶段旨在理解**语义级触觉属性**，并提取**与传感器无关的特征**，从而建立统一的表示空间并减小传感器间的差距。

• **任务一：多模态对比学习对齐 (**Lalign​**)****:**

    ◦ **目的：** 利用配对的视觉图像和触觉属性文本描述作为“桥梁”，将不同传感器的数据绑定在一起，以增强语义级感知。

    ◦ **锚点：** 选择**文本模态**作为锚点，对齐触觉 (T)、视觉 (V) 和文本 (L) 模态。

    ◦ **机制：** 设计了**兼容模态缺失**的对比学习方法，最大化利用批次内每种模态组合的最大子集进行对齐。

    ◦ **实现：** 计算 T-V, T-L 和 V-L 之间的对比损失。

• **任务二：跨传感器匹配 (**Lmatch​**)****:**

    ◦ **目的：** 明确地利用 TacQuad 的多传感器对齐数据，将来自**同一物体同一位置**但由**不同传感器**采集的表示聚集在一起。

    ◦ **机制：** 模型需判断给定的两份触觉数据（XT​ 和 XT+​）是否来自同一物体和位置（即正样本对）。

    ◦ **损失函数：** 使用**二元交叉熵损失**（Binary Cross Entropy Loss）计算匹配分数 m+ 和 m−。

• **第二阶段总损失：** Lstage2​=Lalign​+λLmatch​。

• **训练时长：** **12 个 epochs**。

--------------------------------------------------------------------------------

##### 2.详细测试过程（评估）
###### 1. 静态感知能力评估

该研究使用了一系列下游数据集来测试模型在分类和预测任务上的泛化能力。在评估下游任务时，通常会根据以往研究的惯例对模型进行微调 (Fine-tuning) 或线性探测 (linear probing)。

• **评估目标：**

    ◦ **已知传感器上的已知数据集：** TAG (GelSight 传感器)。

    ◦ **已知传感器上的未见数据集：** Feel (GelSight 传感器)。

    ◦ **未见传感器上的未见数据集：** ObjectFolder 1.0 (TACTO 模拟传感器) 和 ObjectFolder 2.0 (Taxim 模拟传感器)。

• **评估任务：** 材质分类、粗糙度分类、硬度分类和抓取成功率预测。

• **结果分析：** 通过比较在这些数据集上的性能，评估模型将知识从多传感器预训练数据迁移到不同传感器和任务上的能力。

###### 2. 动态感知能力评估（真实世界任务）

为了验证模型在机器人操作中的实时动态感知能力，研究团队进行了**精细倾倒任务**（fine-grained pouring）的实验。

• **任务设置：**

    ◦ **机器人：** 配备 GelSight Mini 传感器的 6-自由度 UFACTORY xArm 6 机械臂。

    ◦ **目标：** 机器人必须仅依靠触觉反馈，精确地从装有 100 克珠子的圆筒中倒出 **60 克**的珠子。

    ◦ **感知挑战：** 任务要求模型分析触觉图像间的精细变化（例如圆筒旋转和珠子倾倒时压力和形变的连续变化），以决定倾倒速度和收回圆筒的最佳时机。

• **评估方法：**

    ◦ 模型通过**模仿学习**进行训练，并进行 10 次真实世界测试运行。

    ◦ **评估指标：** **平均误差**（Mean Error，以克为单位 g），即倒出的质量与目标质量之间的误差。

###### 3. 表示空间和跨传感器生成评估

• **表示空间可视化 (Q2)：** 为了验证 AnyTouch 是否成功地构建了统一的表示空间，研究团队使用了 **t-SNE** 方法。

    ◦ **数据：** 使用 TacQuad 数据集中**未用于预训练**的精细粒度子集（30 次触摸的对齐接触帧）。

    ◦ **目的：** 可视化来自 GelSight Mini、DIGIT 和 DuraGel **同一位置**的触觉表示，观察这些表示是否聚集在一起，从而证明提取了**与传感器无关的特征**。

• **跨传感器生成 (Cross-Sensor Generation)：**

    ◦ **目的：** 进一步展示 TacQuad 数据集和 AnyTouch 提取高质量触觉表示的能力。

    ◦ **任务：** 训练模型根据 GelSight Mini 或 DIGIT 图像生成**对齐的 DuraGel 图像**，或从 GelSight Mini 和 DIGIT 数据中重建 **Tac3D 传感器捕获的力场**。

    ◦ **评估指标：** 生成数据与真实值之间的**均方误差（MSE）**。