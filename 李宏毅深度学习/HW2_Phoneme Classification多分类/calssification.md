##### 任务：
input->找到一个*Fuction*->Class n
##### 引入
>为什么不能当作回归任务来写？

比如Class1 对应 y=1,Class2 对应 y=-1,0是分界线
![[Pasted image 20251108173406.png]]
但是会有一个问题，如下图所示，当有一些Class1的y输出远远超过1的时候就会导致回归模型认为那些输出是error，就会偏离之前正确的判断
>图中绿色的线就表示0分界线，紫色的表示模型为了降低那些error做出的新的回归判断
![[Pasted image 20251108173947.png]]

>图中重复了两个w1x1,以图像中的表达式y=b+w1x1+w2x2为准

 ![[Pasted image 20251108175836.png]]
 ![[Pasted image 20251108180037.png]]
##### 编码类别
 在考虑如何实现分类之前，我们先把各种类别进行一位有效编码即one-hot编码
 ![[Pasted image 20251108202247.png]]
 
 
 关心的是对正确类别的判定置信度大不大
 通过对正确类别和不正确类别的距离拉开来，也就是下面的表达式：
 ![[Pasted image 20251108202040.png]] 
 >y代表正确类别，i代表出正确类别之外的类

##### Softmax
有人会叫Softmax Regression，但是其实是用在多分类任务而不是回归任务
公式如下
![[Pasted image 20251108202850.png]]
举一个‘栗子’：假如有一个向量如下：![[Pasted image 20251108202916.png]]
那么代入分母有：
![[Pasted image 20251108202922.png]]
![[Pasted image 20251108203044.png]]
输出每一类别的置信度：
![[Pasted image 20251108203150.png]]
也可以看看吴恩达的解释，更简单易懂：
![[Pasted image 20251108204821.png]]
![[Pasted image 20251108205154.png]]

其实就是输出每一类别的概率，总和为1.
![[Pasted image 20251108203412.png]]
其中y_hat是有一个1（属于正确的/唯一的类别）,其余都是0，y_hat_i是预测输出的概率，那么如何计算预测和真实的损失？
  用交叉熵函数：
##### 熵的概念
在介绍交叉熵函数之前，我们先试着理解一下信息论中熵的含义和数学表达。
我们可以很容易就知道：不确定性越高，消除这种不确定性所需要的信息量就越大，因此熵就越大。
比如太阳从东方升起的信息量小，因为不确定性低；相反太阳从西方升起的信息量最大，因为不确定性高
![[Pasted image 20251108211023.png]]
![[Pasted image 20251108211058.png]]
  
  ![[Pasted image 20251108212339.png]]
  >P（x_i)是事件发生的概率

  其实就是一个递减关系，确定性越高熵越小
##### 交叉熵
  回到交叉熵，“交叉”熵其实就是指两个概率分布的差异![[Pasted image 20251108213348.png]]
  当我们把p看作是真实标签1，q是预测的分布，会发现公式变成了下面这样：
  ![[Pasted image 20251108213902.png]]
  眼不眼熟？其实就是自信息量的数学表达式
  ![[Pasted image 20251108213946.png]]
  那么函数图像自然是一样的形态。这也很符合我们对损失函数的理解，因为预测概率越大，交叉熵越小，说明损失越小，置信度更高。如果是多分类呢？其实是类似的，因为我们都是将预测的概率分布和真实的做差异比较。
  假如有第二类：![[Pasted image 20251108214201.png]]
  那如何实现对损失函数的梯度下降？下次更。