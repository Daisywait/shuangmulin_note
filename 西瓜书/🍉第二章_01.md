
> [!PDF|] [[周志华-机器学习_.pdf#page=23&selection=40,0,80,5|周志华-机器学习_, p.23]]
> > 通常我们把分类错误的样本数占样本总数的比例称为"错误率" (error rate) ，即如果在 m 个样本中有 α 个样本分类错误，则错误率 E= α 1m; 相应的， 1 一 α 1m 称为"精度" (acc 旧 acy) ，即"精度 =1 一错误率"
> 
>

 ![[Pasted image 20251017153946.png]]

> [!PDF|] [[周志华-机器学习_.pdf#page=23&selection=117,18,119,21|周志华-机器学习_, p.23]]
> > 然而，当学习器把训练样本学得"太好"了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降

> [!PDF|] [[周志华-机器学习_.pdf#page=23&selection=140,18,142,17|周志华-机器学习_, p.23]]
> > 过拟合是无法彻底避免的，我们所能做的只是"缓解'气或者说减小其风险.
> [!PDF|] [[周志华-机器学习_.pdf#page=24&selection=276,1,321,2|周志华-机器学习_, p.24]]
> > 为此， 需使用一个 "测试集 " (testing set) 来测试学习器对新样本的判别能力，然后以测试集上的"测试误差" (testing error) 作为泛化误差的近似
> 
> > [!PDF|] [[周志华-机器学习_.pdf#page=25&selection=13,0,60,1|周志华-机器学习_, p.25]]
> > 可是，我们只有一个包含 m 个样例的数据集 D={( 叫 ， Yl) , (X2 ,Y2) , … 7 (Xm ， Ym)} ， 既要训练，又要测试，怎样才能做到呢?答案是:通过对 D 进行适当的处理，从中产生出训练集 S 和测试集 T.

> [!PDF|] [[周志华-机器学习_.pdf#page=25&selection=65,0,84,1|周志华-机器学习_, p.25]]
> > "留出法" (hold-out) 直接将数据集 D 划分为两个互斥的集合?其中一个集合作为训练集 5 ，另一个作为测试集 T ，

> [!PDF|] [[周志华-机器学习_.pdf#page=25&selection=172,0,174,24|周志华-机器学习_, p.25]]
> > 需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免困数据划分过程引入额外的偏差而对最终结果产生影响

分层采样：

- 先按 “性别” 把学生分成两层（女生层 600 人、男生层 400 人）；
- 再按两层的人数比例来抽样：女生层抽 60 人（600/1000×100）、男生层抽 40 人（400/1000×100）；
- 最后用这 100 个样本算平均身高，就不会因为某类人太少而跑偏。
[!PDF|yellow] [[周志华-机器学习_.pdf#page=25&selection=187,0,251,6&color=yellow|周志华-机器学习_, p.25]]
> > 例如通过对 D 进行分层采样而获得含 70% 样本的训练集 S 和含 30% 样本的测试集 T ， 若 D 包含 500 个正例、 500 个反例，则分层采样得到的 S 应包含 350 个正例、 350 个反例?而 T 则包含 150 个正例和 150 个反例;若 S 、 T 中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差.


> [!PDF|yellow] [[周志华-机器学习_.pdf#page=25&selection=283,3,285,8&color=yellow|周志华-机器学习_, p.25]]
> > ，……这些不同的划分将导致不同的训练/测试集，相应的?模型评估的结果也会有差别.


> [!PDF|yellow] [[周志华-机器学习_.pdf#page=25&selection=285,34,289,9&color=yellow|周志华-机器学习_, p.25]]
> > 在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果

> [!PDF|important] [[周志华-机器学习_.pdf#page=26&selection=97,10,211,1&color=important|周志华-机器学习_, p.26]]
> > 若令训练集 S 包含绝大多数样本 7 则训练出的模型可能更接近于用 D 训练出的模型， 但由于 T 比较小 ，评估结果可能不够稳定准确 ;若令测试集 T 多包含一些样本， 则训练集 S 与 D 差别更大了，被评估的模型与用 D 训练出的模型相比可能有较大差别?从而降低了评估结果的保真性 (fi d e lity) 这个问题没有完美的解决方案 ， 常见做法是将大约 2 / 3 rv 4/5 的

![[周志华-机器学习_.pdf#page=26&rect=159,145,485,307|周志华-机器学习_, p.26]]
> [!PDF|] [[周志华-机器学习_.pdf#page=26&selection=688,4,740,5|周志华-机器学习_, p.26]]
> > 为减小因样本划分不同而引入的差别 ， k 折交叉验证通常要随机使用不同的划分重复 p 次?最终的评估结果是这 p 次 k 折交叉验证结果的均值，例如常见的有 "10 次 10 折交叉验证

> [!PDF|] [[周志华-机器学习_.pdf#page=27&selection=65,5,71,25|周志华-机器学习_, p.27]]
> > 留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用 D 训练出的模型很相似.因此，留一法的评估结果往往被认为比较准确
> 
> > [!PDF|] [[周志华-机器学习_.pdf#page=27&selection=101,25,102,19|周志华-机器学习_, p.27]]
> > 留一法受训练样本规模变化的影响较小，但计算复杂度又太高了
> 
> > [!PDF|] [[周志华-机器学习_.pdf#page=27&selection=125,0,180,17|周志华-机器学习_, p.27]]
> > 给定包含 m 个样本的数据集 D ， 我们对它进行采样产生数据集 D': 每次随机从 D 中挑选一个样本 7 将其拷贝放入 DF' 然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到;这个过程重复执行 m 次后?我们就得到了包含 m 个样本的数据集 DF ，这就是自助采样的结果.显然 ， D 中有一部分样本会在 D' 中多次出现，而另一部分样本不出现.

> [!PDF|] [[周志华-机器学习_.pdf#page=27&selection=38,0,42,4|周志华-机器学习_, p.27]]
> > 自助采样亦称"可重复采样"或"有放回采样

> [!PDF|] [[周志华-机器学习_.pdf#page=27&selection=236,0,236,26|周志华-机器学习_, p.27]]
> > 自助法在数据集较小、难以有效划分训练/测试集时很有用

> [!PDF|yellow] [[周志华-机器学习_.pdf#page=28&selection=8,2,8,27&color=yellow|周志华-机器学习_, p.28]]
> > 在初始数据量足够时，留出法和交叉验证法更常用一些.
> 
> > [!PDF|yellow] [[周志华-机器学习_.pdf#page=28&selection=18,27,24,18&color=yellow|周志华-机器学习_, p.28]]
> > 除了要对适用学习算法进行选择，还需对算法参数进行设定，这就是通常所说的"参数调节"或简称"调参" (parameter tuning)
> 
> > [!PDF|yellow] [[周志华-机器学习_.pdf#page=28&selection=32,15,54,10&color=yellow|周志华-机器学习_, p.28]]
> > 现实中常用的做法?是对每个参数选定一个范围和变化步长，例如在 [0 ， 0.2] 范围内以 0.05 为步长，则实际要评估的候选参数值有 5 个，最终是从这 5 个候选值中产生选定值


> [!PDF|yellow] [[周志华-机器学习_.pdf#page=28&selection=57,14,80,23&color=yellow|周志华-机器学习_, p.28]]
> > 假定算法有 3 个参数，每个参数仅考虑 5 个候选值，这样对每一组训练/测试集就有 53 = 125 个模型需考察;很多强大的学习算法有大量参数需设定，这将导致极大的调参工程量，以至于在不少应用任务中， 例如大型"深度学习" 参数调得好不好往往对最终模型性能有关键性影响.

> [!PDF|yellow] [[周志华-机器学习_.pdf#page=28&selection=118,3,119,23&color=yellow|周志华-机器学习_, p.28]]
> > 在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力

![[周志华-机器学习_.pdf#page=29&rect=174,328,520,473|周志华-机器学习_, p.29]]![[周志华-机器学习_.pdf#page=29&rect=155,23,517,246|周志华-机器学习_, p.29]]![[周志华-机器学习_.pdf#page=30&rect=198,523,505,585|周志华-机器学习_, p.30]]

> [!PDF|yellow] [[周志华-机器学习_.pdf#page=30&selection=55,0,60,30&color=yellow|周志华-机器学习_, p.30]]
> > 类似的需求在信息检索、 Web 搜索等应用中经常出现?例如在信息检索中，我们经常会关心"检索出的信息中有多少比例是用户感兴趣的"

查准率是在预测结果为正的时候模型给出的答案有多少预测对了的，即查准率，准确率。
而精度是分类正确的样本数占样本总数的比例，两者不一样。
查全率（或召回率）和查准率分子是一样的都是预测为正的，分母从预测为正变成真实情况为正

> [!PDF|yellow] [[周志华-机器学习_.pdf#page=30&selection=220,0,222,18&color=yellow|周志华-机器学习_, p.30]]
> > 查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低.

